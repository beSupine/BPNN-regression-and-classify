@uestc-神经网络导论

#一、标准BP算法
1.1 三层BP网络
标准的BP算法只需要使用三层前馈网就可以任意精度地逼近非线性函数，但也存在一些问题，比如容易形成局部极小而得不到最优值、学习效率低、收敛 速度慢等[1]。
标准BP算法的学习过程由正向传播及误差的反向传播两个过程构成。正向传播时输入样本从输入层传入，输出层的实际输出与期望输出比较会得出误差情况，再根据误差情况反向传播调整隐藏层及输出层的输入权值，经过不断地调整，逐步减少输出与期望输出的误差，当减少到误差可接受或已训练到预定的次数时则训练结束。
各种对标准BP算法的改进集中于如何调整权值变更的值上。标准BP神经网络的情况如图1所示。调整权值的原则是使误差不断减小，做法是将权值变更的值与误差的梯度下降成正比。

图1：标准BP神经网络
1.2 学习过程
（1）确定网络结构.
（2）权值及隐层激活函数的参数初始化.
（3）依次输入P个学习样本，设当前输入为第p个样本.
（4）以此计算各层的输入、输出.
（5）求各层权值及激活函数参数的反向传播误差，并记下各值.
（6）记录已学习过样本个数p，若p<P，则转到（3）继续计算；若p=P。则转到（7）.
（7）按权值修正公式各层的权值、阈值和隐层各神经元激活函数的参数.
（8）按新的修正值再计算各层的输入、输出和误差，若误差小于所要求的精度或达到最大学习次数，则终值学习，否则转到（3）继续新一轮的学习[2].
1.3 数学推导
设标准BP神经网络的输入向量为X，输出向量为Y，期望输出向量为D， Y向量中元素的个数为I，训练的目标是使误差E极小，定义误差E的目标函数为[3]：
E=1/2 (D-Y)^2=1/2 ∑_(k=1)^1▒(d_k-y_k )^2 	(1)
以3层BP神经网络为例分析，设隐藏层的权值矩阵为V，即（V_1,···,V_j,···,V_m）;输出层的权值矩阵为W，即（W_1,···,W_k,···,W_i），η为学习率，转移函数表示为：
f(X)=f(net)=f(∑vX)	(2)
则有：
∆W_(j,k)=-η ∂E/(∂〖net〗_k )  (∂〖net〗_k)/(∂W_(j,k) )	(3)
∆V_(i,j)=-η ∂E/(∂V_(i,j) )=-η ∂E/(∂〖net〗_j )  (∂〖net〗_j)/(∂V_(i,j) )	(4)
令δ的值为误差相对网络的偏导的负值，表示输出层和隐层的误差信号，可得：
δ_k^o=-∂E/(∂〖net〗_k )	(5)
δ_j^y=-∂E/(∂〖net〗_j )	(6)
可以通过偏导计算得出：
∆W_(j,k)=ηδ_k^σ 〖Wy〗_i=η(d_k-o_k)o_k (1-o_k)y_i	(7)
∆V_(i,j)=ηδ_j^y x_i=η(∑_(k=1)^1▒δ_k^o  W_(j,k) )j(1-y_i)x_i	(8)
#二、BP网络的优化方法
2.1 随机梯度下降算法（SGD）
随机梯度下降（SGD）是一种简单但非常有效的方法，多用用于支持向量机、逻辑回归等凸损失函数下的线性分类器的学习。并且SGD已成功应用于文本分类和自然语言处理中经常遇到的大规模和稀疏机器学习问题。
SGD既可以用于分类计算，也可以用于回归计算。
SGD算法是从样本中随机抽出一组，训练后按梯度更新一次，重复此步骤直到损失函数的值在可接受的范围。在每次的迭代过程种，样本都要被随机打乱，以有效减小样本之间造成的参数更新抵消问题。在样本量极其大的情况下，可能不用训练完所有的样本就可以获得一个损失值在可接受范围之内的模型。
用η表示学习率，W表示需要更新的权重参数，损失函数关于W的梯度记为∂L/∂W，则参数更新可表示为：
W←W-η ∂L/∂W	(7)
随机梯度下降算法通常还有三种不同的应用方式，它们分别是SGD、批随机梯度下降（Batch-SGD）、（小批量随机梯度下降）Mini-Batch SGD，本文只讨论SGD。
SGD是一种常用的BP网络的参数更新方法，但它有许多缺点，例如选择合适的学习率比较困难、容易收敛到局部最优，在某些情况下可能被困在鞍点。正因为有这些缺点后续才发展出了各种算法。
2.2 Momentum算法
Momentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度。用数学公式表示Momentum方法[4]：
v←αv-η ∂L/∂W	(8)
W←W+v	(9)
和SGD一样，η表示学习率，W表示需要更新的权重参数，损失函数关于W的梯度为∂L/∂W。v表示物理上的速度。
式(8)中αv这一项，在物体不受任何力时，该项承担使物体逐渐减速的任务（α设置为0.9之类的值），对应物理上的地面摩擦或空气阻力。
Momentum算法是对SGD的一种优化，其动量项能够在相关方向加速SGD，抑制振荡，从而加快收敛。
2.3 AdaGrad算法
在神经网络的学习中，学习率η的值很重要。学习率过小会导致学习花费的时间过多；反过来，学习率过大，则会导致学习发散而不能正确进行。
有一种学习率衰减的方法，可以使学习率随着学习的进行逐渐减小。
AdaGrad方法会为参数的每个元素适当地调整学习率，与此同时进行学习。
用数学公式表示AdaGrad方法：
h←h+η ∂L/∂W·∂L/∂W	(10)
W←W-η 1/√h  ∂L/∂W	(11)
η表示学习率，W表示需要更新的权重参数，损失函数关于W的梯度为∂L/∂W，变量h保存了以前地所有梯度值地平方和，然后在更新参数时，通过乘以1/√h，就可以调整学习地尺度。这意味着参数地元素中变动较大地元素地学习率将变小。即是可以按照参数的元素进行学习率衰减，使变动答得参数得学习率逐渐减小。但是在训练的中后期，分母上梯度平方的累加将会越来越大，使梯度趋近于0，使得训练提前结束。
2.4 RMSprop算法
RMSprop算法与AdaGrad算法和相似，二者的不同之处在于累积平方梯度的求法不同。RMSProp算法不像AdaGrad算法那样直接的累加平方梯度，而是加了一个衰减系数来控制历史信息的获取。
同样，我们使用η表示学习率，W表示需要更新的权重参数，损失函数关于W的梯度为∂L/∂W，ρ表示衰减率，用数学公式表示AdaGrad方法：
h←hρ	(12)
h←h+(1-ρ)∂L/∂W·∂L/∂W	(13)
W←W-η 1/√h  ∂L/∂W	(14)
RMSprop算法仍然依赖于全局学习率，它适合处理非平稳目标，在训练后期，反复在局部最小值附近抖动，利于跳出局部最小值。
2.5 Adam算法
Adam(Adaptive Moment Estimation)算法本质上是带有动量项的RMSprop算法，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。
同样使用η表示学习率，W表示需要更新的权重参数，损失函数关于W的梯度为∂L/∂W。
m_t←β_1 m_(t-1)+(1-β_1)∂L/∂W	(15)
n_t←β_2 n_(t-1)+(1-β_2 )  ∂L/∂W·∂L/∂W	(16)
(m_t ) ̂←m_t/(1-〖β_1〗^t )	(17)
(n_t ) ̂←n_t/(1-〖β_2〗^t )	(18)
W←W-η (m_t ) ̂/√((n_t ) ̂ )	(19)
m_t与n_t分别是对梯度的一阶矩估计和二阶矩估计，(m_t ) ̂与(n_t ) ̂是对m_t、n_t的偏置校正，β_1与β_2分别为一阶矩估计和二阶矩估计的衰减率。
Adam算法结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点，可以为不同的参数计算不同的自适应学习率，适用于大数据集和高维空间。
#三、AQI指数预测
AQI指数是定量描述空气质量状况的指数，其数值越大说明空气污染状况越严重，对人体健康的危害也就越大。参与空气质量评价的主要污染物为细颗粒物（PM2.5）、可吸入颗粒物（PM10）、二氧化硫（SO2）、二氧化氮（NO2）、臭氧（O3）、一氧化碳（CO）等六项。本文以这个六个因素作为神经网络的输入，AQI指数作为输出结果，分别实现这5种改进算法，并对比研究。
表 1 成都市空气质量数据（摘选）
DATE	AQI	LEVEL	PM2.5	PM10	SO2	CO	NO2	O3_8h
2020/1/1	135	轻度污染	103	124	6	0.9	45	27
2020/1/2	135	轻度污染	103	132	10	1	57	22
2020/1/3	105	轻度污染	79	102	6	1	57	56
2020/1/4	118	轻度污染	89	119	8	1.1	67	16
2020/1/5	129	轻度污染	98	124	6	0.9	52	18
2020/1/6	120	轻度污染	91	116	8	1.2	61	17
2020/1/7	78	良	57	85	7	1	53	71
仿真实验过程中采用三层神经网络，其中第一层为输入层，将6种主要污染物作为输入，故憨厚6各输入神经元；第二层为隐层，目前对于隐层的神经元个数的确定，还没有形成一套完整的理论，本文采用比较通用的计算方法m=√(n+l)+α，其中n为输入层节点个数，l为输出层节点个数，α是1~10之间的数，m为隐层神经元个数，本文确定为9个隐层神经元；第三层为输出层，含有1个神经元，即为预测得到的AQI指数；由以上分析得到神经网络的结构为6 - 9 – 1。传递函数采用tanh函数，函数形式如下：
tanh⁡(x)=2/(1+e^(-2x) )-1	(20)
选取成都市2020年1月初至2020年5月底共150个空气质量数据，所有训练数据都进行归一化处理，统一各输入量的数量级，避免各输入量不同单位带来的影响。其中前120个数据用于神经网络的训练，后30个数据用于测试，即前4个月的数据用于训练，后1个月的数据用于测试。利用Python和PyTorch建立5个BP神经网络，进行网络训练与测试，`每个BP网络都具有相同的网络结构，只有参数更新算法不同`。
学习率统一取0.01，Momentum的动量取0.9，RMSprop的衰减率取0.99，Adam的一阶矩估计和二阶矩估计的衰减率取0.9和0.999，迭代次数统一取5000次，5000次的迭代可以使这5个算法的梯度都趋于平稳。
对SGD、Momentum、AdaGrad、RMSprop和Adam的BP算法进行模拟仿真和对比，结果如下表：
表 2 仿真结果对比表
算法	训练均方误差	测试均方误差	平均偏差
SGD	0.0098	0.0088	0.0717
Momentum	0.0060	0.0079	0.0673
AdaGrad	0.0023	0.0066	0.0677
RMSprop	0.0010	0.0052	0.0618
Adam	0.0002	0.0043	0.0560
以上结果表明，BP网络预测的结果与期望输出值很接近，5个算法都取得了较好的结果，且Adam算法的效果最好，其他4个算法的效果RMSprop > AdaGrad > Momentum > SGD.
